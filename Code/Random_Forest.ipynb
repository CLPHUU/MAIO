{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# general\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# for RF\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# for stats\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib.patches import Patch\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# global variables\n",
    "DPI = 200\n",
    "N_iter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data: look at street\n",
    "data_folder = os.path.join('..', 'Data')\n",
    "file_name = 'Street_NL10236-AQ-METEO.csv' #'Urban_NL10418-AQ-METEO.csv' #'Rural_NL10644-AQ-METEO.csv'\n",
    "data_imported = pd.read_csv(os.path.join(data_folder, file_name), sep=\";\")\n",
    "variable = 'no2' # which variable do we want to predict\n",
    "\n",
    "# prepare dates\n",
    "data_imported['date'] = pd.to_datetime(data_imported['date'])\n",
    "\n",
    "# select only meterological variables + date + look at selected variable \n",
    "met_vars_initial = ['wd', 'ws', 't', 'q', 'hourly_rain', 'p', 'n', 'rh']\n",
    "\n",
    "selected_data = data_imported[met_vars_initial + ['date'] + [variable]]\n",
    "N_initial = selected_data.shape[0]\n",
    "selected_data = selected_data.dropna()\n",
    "N_cleaned = selected_data.shape[0]\n",
    "selected_data.loc[selected_data[variable] < 0, variable] = 0\n",
    "\n",
    "print((1 - N_cleaned/N_initial)*100, 'Percent of the data was removed due to missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "new_column_names = {\n",
    "    'wd': 'Wind Direction',\n",
    "    'ws': 'Wind Speed',\n",
    "    't': 'Temperature',\n",
    "    'q': 'Specific Humidity',\n",
    "    'hourly_rain': 'Hourly Rainfall',\n",
    "    'p': 'Pressure',\n",
    "    'n': 'Cloud Coverage',\n",
    "    'rh': 'Relative Humidity'\n",
    "}\n",
    "\n",
    "selected_data.rename(columns=new_column_names, inplace=True)\n",
    "met_vars = [new_column_names[var] for var in met_vars_initial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(selected_data, vars, variable):\n",
    "    predictors = selected_data[vars]\n",
    "    variable_data = selected_data[variable]\n",
    "\n",
    "    train_mask = selected_data['date'].dt.year <= 2017\n",
    "    test_mask = selected_data['date'].dt.year > 2017\n",
    "\n",
    "    X_train, y_train = predictors[train_mask], variable_data[train_mask]\n",
    "    X_test, y_test = predictors[test_mask], variable_data[test_mask]\n",
    "\n",
    "    t_train = selected_data['date'][train_mask]\n",
    "    t_test = selected_data['date'][test_mask]\n",
    "\n",
    "    return X_train, y_train, t_train, X_test, y_test, t_test\n",
    "\n",
    "X_train, y_train, t_train, X_test, y_test, t_test = prepare_data_for_training(selected_data, met_vars, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=DPI, figsize=(10, 4))\n",
    "plt.scatter(t_train, y_train, label='Train', s=0.1)\n",
    "plt.scatter(t_test, y_test, label='Test', s=0.1)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(r'[$\\text{NO}_2$] ($\\mu g/\\text{m}^3$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_optimization(X_train, y_train, n_iter=20, random_state=None):\n",
    "    # Define the hyperparameter grid\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(20, 200),\n",
    "        'min_samples_leaf': randint(3, 10),\n",
    "        'max_samples': uniform(0, 0.9),    \n",
    "        'max_features': uniform(0, 1),\n",
    "        'max_depth': randint(2, 20),\n",
    "    }\n",
    "\n",
    "    # Create the RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "    # Create the RandomizedSearchCV instance\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=rf_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,  # Number of random combinations to try\n",
    "        cv=5,       # Number of cross-validation folds --> no separate validation data needed\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Fit the RandomizedSearchCV instance to your data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = random_search.best_params_\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Get the best model\n",
    "    best_rf_model = random_search.best_estimator_\n",
    "\n",
    "    return best_rf_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model, best_params = RF_optimization(X_train, y_train, n_iter=N_iter)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_train_pred = best_rf_model.predict(X_train)\n",
    "y_test_pred = best_rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tree_number(params, N_tree=11, n_runs=1, plot=True, random_state=None):\n",
    "    n_tree_test_arr = [2**n for n in range(N_tree)]\n",
    "    train_rmse_list = []\n",
    "    test_rmse_list = []\n",
    "\n",
    "    for n_trees_test in n_tree_test_arr:\n",
    "        train_rmse_accumulator = 0\n",
    "        test_rmse_accumulator = 0\n",
    "\n",
    "        for _ in range(n_runs):\n",
    "            test_params = params.copy()\n",
    "            test_params['n_estimators'] = n_trees_test\n",
    "            \n",
    "            # Train a model using the test_params\n",
    "            rf_test_model = RandomForestRegressor(**test_params, random_state=random_state)\n",
    "            rf_test_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_train_pred = rf_test_model.predict(X_train)\n",
    "            y_test_pred = rf_test_model.predict(X_test)\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "            \n",
    "            # Accumulate RMSE for averaging\n",
    "            train_rmse_accumulator += train_rmse\n",
    "            test_rmse_accumulator += test_rmse\n",
    "        \n",
    "        # Calculate the average RMSE and append it to the results list\n",
    "        train_rmse_list.append(train_rmse_accumulator / n_runs)\n",
    "        test_rmse_list.append(test_rmse_accumulator / n_runs)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6), dpi=DPI)\n",
    "        plt.plot(n_tree_test_arr, train_rmse_list, label='Training RMSE', marker='o')\n",
    "        plt.plot(n_tree_test_arr, test_rmse_list, label='Test RMSE', marker='o')\n",
    "        plt.xscale('log')  # because n_trees_test are powers of 2\n",
    "        plt.xlabel('Number of Trees')\n",
    "        plt.ylabel('RMSE ($\\mu g/\\text{m}^3$)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return n_tree_test_arr, train_rmse_list, test_rmse_list\n",
    "\n",
    "if True:\n",
    "    check_tree_number(best_params, N_tree=11, n_runs=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_pred(y_train, y_train_pred, y_test, y_test_pred):\n",
    "    fig, ax1 = plt.subplots(1, 1, dpi=DPI, figsize=(10, 5))\n",
    "    \n",
    "    # Scatter plots\n",
    "    size = 1\n",
    "    transp = 1\n",
    "    ax1.scatter(y_train, y_train_pred, label='Train', color='tab:blue', s=size, alpha=transp)\n",
    "    ax1.scatter(y_test, y_test_pred, label='Test', color='tab:orange', s=size, alpha=transp)\n",
    "\n",
    "    # 1:1 line\n",
    "    line = np.array([0, np.max([np.max(y_train_pred), np.max(y_test_pred)])])\n",
    "    ax1.plot(line, line, zorder=5, linestyle='dashed', color='black', label='1:1 line', alpha=.7)\n",
    "    ax1.set_aspect('equal', 'box')\n",
    "\n",
    "    ax1.set_xlabel(r'Measured [$\\text{NO}_2$] ($\\mu g/\\text{m}^3$)')\n",
    "    ax1.set_ylabel(r'Predicted [$\\text{NO}_2$] ($\\mu g/\\text{m}^3$)')\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "    \n",
    "    plt.show()\n",
    "plot_model_pred(y_train, y_train_pred, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your function\n",
    "def plot_model_pred_timeseries(t_train, y_train, y_train_pred, t_test, y_test, y_test_pred, all=True, start_month=1, n_months=3, start_day=1, n_days=3, residuals=False):\n",
    "    if not residuals:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, dpi=DPI, figsize=(10, 4), sharey=True)\n",
    "    else:\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, dpi=DPI, figsize=(10, 10))\n",
    "\n",
    "    ax1.plot(t_train, y_train, alpha=.5, label='Observation', color='tab:blue')\n",
    "    ax1.plot(t_train, y_train_pred, alpha=.5, label='Prediction', color=\"tab:purple\")\n",
    "    ax1.set_ylim(0, 120)\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True)\n",
    "    ax1.set_ylabel(r'[$\\text{NO}_2$] ($\\mu g/\\text{m}^3$)')\n",
    "    ax1.set_title('Training Set')\n",
    "\n",
    "    ax2.plot(t_test, y_test, alpha=.5, label='Observation', color='tab:orange')\n",
    "    ax2.plot(t_test, y_test_pred, alpha=.5, label='Prediction', color=\"tab:red\")\n",
    "    ax2.grid(True)\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel(r'[$\\text{NO}_2$] ($\\mu g/\\text{m}^3$)')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_title('Test Set')\n",
    "\n",
    "    if not all:\n",
    "        ax1.set_xlim(datetime.date(2017, start_month, start_day), datetime.date(2017, start_month + n_months, start_day + n_days))\n",
    "        ax2.set_xlim(datetime.date(2018, start_month, start_day), datetime.date(2018, start_month + n_months, start_day + n_days))\n",
    "\n",
    "    if residuals:\n",
    "        # Calculate residuals\n",
    "        train_residuals = y_train - y_train_pred\n",
    "        test_residuals = y_test - y_test_pred\n",
    "\n",
    "        # Residuals plot\n",
    "        ax3.plot(t_train, train_residuals, alpha=.5, label='Train Residuals')\n",
    "        ax3.plot(t_test, test_residuals, alpha=.5, label='Test Residuals')\n",
    "        ax3.grid(True)\n",
    "        ax3.legend()\n",
    "        ax3.set_ylabel(r'Residuals ($\\mu g/\\text{m}^3$)')\n",
    "\n",
    "        # Calculate and plot ACF, then perform Fourier analysis\n",
    "        autocorr = acf(train_residuals, nlags=24*365, fft=True) # ACF calculation\n",
    "        spectrum = np.fft.fft(autocorr)  # FFT of ACF\n",
    "        freq = np.fft.fftfreq(len(spectrum)) \n",
    "\n",
    "        # ACF plot of residuals\n",
    "        x = np.arange(len(autocorr)) / 24 # Convert lag to days\n",
    "        ax4.plot(x, autocorr)\n",
    "        ax4.set_xlabel('Lag [days]')\n",
    "        ax4.set_ylabel('Autocorrelation')\n",
    "        ax4.grid(True)\n",
    "        ax4.set_yscale('log')\n",
    "        ax4.set_ylim(1e-3, 1)\n",
    "\n",
    "        # For visualization, usually just half of the spectrum is enough\n",
    "        pos_mask = np.where(freq > 0)\n",
    "        freqs = freq[pos_mask]\n",
    "        periods = 1 / freqs # Convert frequency to period in days\n",
    "        power = np.abs(spectrum[pos_mask])**2\n",
    "        power_normalized = power / np.sum(power)\n",
    "\n",
    "        ax5.loglog(periods, power_normalized)  # Plot the power spectrum of ACF\n",
    "        ax5.set_xlabel('Period [days]')\n",
    "        ax5.set_ylabel('Power')\n",
    "        ax5.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_model_pred_timeseries(t_train, y_train, y_train_pred, t_test, y_test, y_test_pred, all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_pred_timeseries(t_train, y_train, y_train_pred, t_test, y_test, y_test_pred, all=False, start_month=1, n_months=3, start_day=1, n_days=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RF_feature_imp(model, X_train, plot=False, use_permutation=False, target=None, random_state=None, decimals=2, relative_threshold = 0.1):\n",
    "    '''Calculates feature importance using Gini importance or permutation importance.\n",
    "    \n",
    "    Args:\n",
    "        model (object): The trained Random Forest model.\n",
    "        X_train (DataFrame): The training data features.\n",
    "        plot (bool, optional): Whether to plot the feature importances. Default is False.\n",
    "        use_permutation (bool, optional): If True, calculate permutation importance. \n",
    "            If False (default), calculate Gini importance.\n",
    "        random_state (int or None, optional): Random seed for permutation importance. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing feature importances.\n",
    "    '''\n",
    "    if use_permutation:\n",
    "        result = permutation_importance(model, X_train, target, random_state=random_state)\n",
    "        importances_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': result.importances_mean})\n",
    "    else:\n",
    "        feature_importances = model.feature_importances_\n",
    "        importances_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "\n",
    "    importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))  # Set figure size\n",
    "        bars = plt.barh(importances_df['Feature'], importances_df['Importance'], color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance at the top\n",
    "        plt.grid(axis='x')  # Add a grid along the x-axis\n",
    "        plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "        plt.xscale('log')\n",
    "        \n",
    "        # Annotating the bars\n",
    "        thresh = relative_threshold * importances_df.max().iloc[1]\n",
    "        for bar in bars:\n",
    "            # If bar width is smaller than a threshold, adjust text position and alignment\n",
    "            if bar.get_width() < thresh:  # example threshold\n",
    "                plt.annotate(f\"{bar.get_width():.{decimals}f}\",\n",
    "                            xy=(bar.get_width(), bar.get_y() + bar.get_height()/2),\n",
    "                            xytext=(5, 0),  # adjust the position of text to avoid overlap with bars\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='left', va='center',\n",
    "                            fontsize=8, color='black')\n",
    "            else:\n",
    "                plt.annotate(f\"{bar.get_width():.{decimals}f}\",\n",
    "                            xy=(bar.get_width(), bar.get_y() + bar.get_height()/2),\n",
    "                            xytext=(-5, 0),  # adjust the position of text to avoid overlap with bars\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='right', va='center',\n",
    "                            fontsize=8, color='black')\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "    return importances_df\n",
    "\n",
    "        \n",
    "get_RF_feature_imp(best_rf_model, X_train, plot=True, use_permutation=False, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RF_stats(y_train, y_test, y_train_pred, y_test_pred):\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    correlation_train, _ = pearsonr(y_train, y_train_pred)\n",
    "    correlation_test, _ = pearsonr(y_test, y_test_pred)\n",
    "\n",
    "    explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "    explained_variance_test = explained_variance_score(y_test, y_test_pred)\n",
    "\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    results = {\n",
    "        'RMSE Train': rmse_train,\n",
    "        'RMSE Test': rmse_test,\n",
    "        'Correlation Train': correlation_train,\n",
    "        'Correlation Test': correlation_test,\n",
    "        'Explained Variance Train': explained_variance_train,\n",
    "        'Explained Variance Test': explained_variance_test,\n",
    "        'MAE Train': mae_train,\n",
    "        'MAE Test': mae_test\n",
    "    }\n",
    "\n",
    "    latex_table = \"\\\\begin{tabular}{lcc}\\n\\\\toprule\\n\"\n",
    "    latex_table += \"Metric & Train & Test \\\\\\\\ \\\\midrule\\n\"\n",
    "\n",
    "    metrics_train = [key for key in results.keys() if \"Train\" in key]\n",
    "    metrics_test = [key.replace(\"Train\", \"Test\") for key in metrics_train]\n",
    "\n",
    "    for m_train, m_test in zip(metrics_train, metrics_test):\n",
    "        unit = \" ($\\\\mu g/m^3$)\" if \"RMSE\" in m_train or \"MAE\" in m_train else \"\"\n",
    "        latex_table += f\"{m_train.replace(' Train', '')}{unit} & {results[m_train]:.2f} & {results[m_test]:.2f} \\\\\\\\ \\n\"\n",
    "\n",
    "    latex_table += \"\\\\bottomrule\\n\\\\end{tabular}\"\n",
    "    \n",
    "    print(latex_table)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "RF_stats_1 = get_RF_stats(y_train, y_test, y_train_pred, y_test_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra explainers\n",
    "selected_data_2 = data_imported[met_vars_initial + ['date'] + [variable]].dropna()\n",
    "selected_data_2.rename(columns=new_column_names, inplace=True)\n",
    "selected_data_2.loc[selected_data_2[variable] < 0, variable] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_winter_time(row):\n",
    "    year = row['date'].year\n",
    "    # DST starts on the last Sunday of March\n",
    "    dst_start = max([day for day in pd.date_range(f\"{year}-03-01\", f\"{year}-03-31\") if day.weekday() == 6])\n",
    "    # DST ends on the last Sunday of October\n",
    "    dst_end = max([day for day in pd.date_range(f\"{year}-10-01\", f\"{year}-10-31\") if day.weekday() == 6])\n",
    "    # Return 1 if date is outside DST period, 0 otherwise\n",
    "    return 1 if row['date'] < dst_start or row['date'] > dst_end else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_extra_explainers(data):\n",
    "    # add more variables that could explain emissions\n",
    "    dutch_holidays = [\n",
    "        datetime.date(datetime.MINYEAR, 1, 1),   # New Year's Day\n",
    "        datetime.date(datetime.MINYEAR, 4, 27),  # King's Day\n",
    "        datetime.date(datetime.MINYEAR, 5, 5),   # Liberation Day\n",
    "        datetime.date(datetime.MINYEAR, 5, 25),  # Ascension Day\n",
    "        datetime.date(datetime.MINYEAR, 6, 5),   # Whit Monday\n",
    "        datetime.date(datetime.MINYEAR, 12, 25), # Christmas Day\n",
    "        datetime.date(datetime.MINYEAR, 12, 26), # Boxing Day\n",
    "    ]\n",
    "\n",
    "    data['Hour of Day'] = data['date'].dt.hour\n",
    "    data['Day of Year'] = data['date'].dt.dayofyear\n",
    "    data['Day of Week'] = data['date'].dt.dayofweek\n",
    "    data['day_of_month'] = data['date'].dt.day \n",
    "    data['week_of_year'] = data['date'].dt.isocalendar().week\n",
    "\n",
    "    data['month_day'] = data['date'].dt.strftime('%m-%d')\n",
    "    data['holiday'] = data['month_day'].isin(date.strftime('%m-%d') for date in dutch_holidays).astype(int)\n",
    "    data.drop(columns=['month_day'], inplace=True)\n",
    "    data['holiday']\n",
    "\n",
    "    data['Winter Time'] = data.apply(is_winter_time, axis=1)\n",
    "    data['working_day'] = ((data['Day of Week'] >= 0) & (data['Day of Week'] <= 4)).astype(int)\n",
    "    return data\n",
    "\n",
    "extra_explainers = ['Hour of Day', 'Day of Year', 'Day of Week']#, 'working_day', 'week_of_year', 'holiday', 'day_of_month', 'Winter Time]\n",
    "get_extra_explainers(selected_data_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, y_train_2, t_train_2, X_test_2, y_test_2, t_test_2 = prepare_data_for_training(selected_data_2, met_vars + extra_explainers, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model_2, best_params_2 = RF_optimization(X_train_2, y_train_2, n_iter=N_iter)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_train_pred_2 = best_rf_model_2.predict(X_train_2)\n",
    "y_test_pred_2 = best_rf_model_2.predict(X_test_2)\n",
    "\n",
    "save_dict = {'y_train_pred': y_train_pred_2, 'y_test_pred': y_test_pred_2}\n",
    "with open(os.path.join('..', 'Model_results','RF2.pkl'), 'wb') as f:\n",
    "    pickle.dump(save_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_pred(y_train_2, y_train_pred_2, y_test_2, y_test_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_pred_timeseries(t_train_2, y_train_2, y_train_pred_2, t_test_2, y_test_2, y_test_pred_2, all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_pred_timeseries(t_train_2, y_train_2, y_train_pred_2, t_test_2, y_test_2, y_test_pred_2, all=False, start_month=1, n_months=3, start_day=1, n_days=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_RF_feature_imp(best_rf_model_2, X_train_2, plot=True, decimals=2, relative_threshold=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_stats_2 = get_RF_stats(y_train_2, y_test_2, y_train_pred_2, y_test_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rf_stats_relative(RF_stats_1, RF_stats_2):\n",
    "    stat_names = [\n",
    "    'RMSE Train',\n",
    "    'RMSE Test',\n",
    "    'Correlation Train',\n",
    "    'Correlation Test',\n",
    "    'Explained Variance Train',\n",
    "    'Explained Variance Test',\n",
    "    'MAE Train',\n",
    "    'MAE Test'\n",
    "    ]\n",
    "    \n",
    "    RF1_stats = np.array([RF_stats_1[name] for name in stat_names])\n",
    "    RF2_stats = np.array([RF_stats_2[name] for name in stat_names])\n",
    "    \n",
    "    # Calculating the relative differences\n",
    "    relative_differences = ((RF2_stats - RF1_stats) / RF1_stats) * 100 \n",
    "    \n",
    "    # Stripping \" Train\" and \" Test\" from labels for x-axis\n",
    "    labels = [name.replace(\" Train\", \"\").replace(\" Test\", \"\") for name in stat_names]\n",
    "\n",
    "    # Get unique labels\n",
    "    unique_labels = []\n",
    "    [unique_labels.append(label) for label in labels if label not in unique_labels]\n",
    "    \n",
    "    x = np.arange(len(unique_labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Differentiating between train and test stats by color\n",
    "    train_color = 'tab:blue'\n",
    "    test_color = 'tab:orange'\n",
    "    colors = [train_color if 'Train' in name else test_color for name in stat_names]\n",
    "\n",
    "    # Plotting Train and Test bars side by side\n",
    "    for i, (label, diff, color) in enumerate(zip(labels, relative_differences, colors)):\n",
    "        bar = ax.bar(x[i//2] + (i%2) * width, diff, width, color=color)\n",
    "        \n",
    "        # Adding percentage above the bars\n",
    "        height = bar[0].get_height()\n",
    "        ax.annotate(f'{diff:.1f}%',\n",
    "                    xy=(bar[0].get_x() + bar[0].get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # Adding a zero line\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Relative Difference (%)')\n",
    "    ax.set_xticks(x + width/2)  # position x-axis labels in the center of grouped bars\n",
    "    ax.set_xticklabels(unique_labels)\n",
    "    \n",
    "    # Creating a custom legend\n",
    "    legend_elements = [Patch(facecolor=train_color, label='Train'),\n",
    "                       Patch(facecolor=test_color, label='Test')]\n",
    "    ax.legend(handles=legend_elements)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_rf_stats_relative(RF_stats_1, RF_stats_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra components\n",
    "comp_vars = ['o3', 'ox']\n",
    "selected_data_3 = data_imported[met_vars_initial + comp_vars + ['date'] + [variable]].dropna()\n",
    "selected_data_3.rename(columns=new_column_names, inplace=True)\n",
    "selected_data_3.loc[selected_data_3[variable] < 0, variable] = 0\n",
    "get_extra_explainers(selected_data_3)\n",
    "X_train_3, y_train_3, t_train_3, X_test_3, y_test_3, t_test_3 = prepare_data_for_training(selected_data_3, met_vars + comp_vars + extra_explainers, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model_3, best_params_3 = RF_optimization(X_train_3, y_train_3, n_iter=N_iter)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_train_pred_3 = best_rf_model_3.predict(X_train_3)\n",
    "y_test_pred_3 = best_rf_model_3.predict(X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_pred(y_train_3, y_train_pred_3, y_test_3, y_test_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_pred_timeseries(t_train_3, y_train_3, y_train_pred_3, t_test_3, y_test_3, y_test_pred_3, all=True)\n",
    "plot_model_pred_timeseries(t_train_3, y_train_3, y_train_pred_3, t_test_3, y_test_3, y_test_pred_3, all=False, start_month=1, n_months=3, start_day=1, n_days=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_RF_feature_imp(best_rf_model_3, X_train_3, plot=True, decimals=4, relative_threshold=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_stats_3 = get_RF_stats(y_train_3, y_test_3, y_train_pred_3, y_test_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_rf_stats_relative(RF_stats_1, RF_stats_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
